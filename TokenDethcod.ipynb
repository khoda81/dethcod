{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token based DETHCOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eSX4vKTl97pS",
    "outputId": "64d713f1-32ad-4db4-ef37-e338a7a4e841",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - conda-forge\n",
      " - defaults\n",
      "Platform: linux-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%conda install -c conda-forge transformers wandb requests_cache datasets tqdm python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3yDIICSsnFOb"
   },
   "source": [
    "## Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JQb9wuBJnFOc",
    "outputId": "e2d53ae0-a602-4e1a-cc30-929920b959dd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|████████████████████████████████████████| 36.4M/36.4M [00:00<00:00, 655MB/s]\n",
      "File downloaded and decompressed successfully.\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import os\n",
    "import sys\n",
    "import zipfile\n",
    "\n",
    "import requests\n",
    "import requests_cache\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "zip_link = \"http://www.mattmahoney.net/dc/enwik8.zip\"\n",
    "data_folder = \"dataset\"\n",
    "cache_file = \"download_cache\"\n",
    "\n",
    "# Ensure the data folder exists\n",
    "if not os.path.exists(data_folder):\n",
    "    os.makedirs(data_folder)\n",
    "\n",
    "# Initialize requests_cache\n",
    "requests_cache.install_cache(os.path.join(data_folder, cache_file))\n",
    "\n",
    "# Download the ZIP file with progress bar\n",
    "response = requests.get(zip_link, stream=True)\n",
    "response.raise_for_status()\n",
    "\n",
    "# Get the total file size for the progress bar\n",
    "total_size = int(response.headers.get(\"content-length\", 0))\n",
    "\n",
    "# Open the ZIP file from the content\n",
    "with open(os.path.join(data_folder, \"enwik8.zip\"), \"wb\") as file:\n",
    "    with tqdm(\n",
    "        total=total_size, unit=\"B\", unit_scale=True, desc=\"Downloading\"\n",
    "    ) as pbar:\n",
    "        for data in response.iter_content(chunk_size=1024):\n",
    "            file.write(data)\n",
    "            pbar.update(len(data))\n",
    "\n",
    "# Open the cached file\n",
    "with open(os.path.join(data_folder, \"enwik8.zip\"), \"rb\") as file:\n",
    "    # Open the ZIP file from the content\n",
    "    with zipfile.ZipFile(io.BytesIO(file.read())) as zip_file:\n",
    "        # Extract all contents to the data folder\n",
    "        zip_file.extractall(data_folder)\n",
    "\n",
    "print(\"File downloaded and decompressed successfully.\", file=sys.stderr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NMCRynUDpAz6"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "BF26H2PapAjj"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"text\", data_files=[\"dataset/enwik8\"])\n",
    "dataset = dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pY1_Ux8uprdh",
    "outputId": "c2b8a9e8-08bd-4e6a-a051-9250780cc27b"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "MODEL_ID = \"google-t5/t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "dZXhU0AfhrTJ"
   },
   "outputs": [],
   "source": [
    "# Removing large and empty samples\n",
    "MAX_LENGTH = 128\n",
    "\n",
    "def filter_samples(example):\n",
    "    tokenized = tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH + 1,\n",
    "        return_attention_mask=False,\n",
    "        return_length=True,\n",
    "    )\n",
    "\n",
    "    return [\n",
    "        1 < sample_length <= MAX_LENGTH\n",
    "        for sample_length in tokenized.length\n",
    "    ]\n",
    "\n",
    "dataset = dataset.filter(filter_samples, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WPWUhHX8A43h",
    "outputId": "099a0d8a-60f5-435b-fdc4-e135e3c4ff93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'== Gestapo counterintelligence =='\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "sample = random.choice(dataset)\n",
    "print(repr(sample[\"text\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wrDpshHUnFOd"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "fGqAZ6NY-FrU"
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "import transformers.modeling_outputs\n",
    "\n",
    "\n",
    "class CompressionConfig(transformers.T5Config): ...\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CompressionOutput(transformers.modeling_outputs.Seq2SeqLMOutput):\n",
    "    value_predictions: Optional[Tuple[torch.FloatTensor, ...]] = None\n",
    "\n",
    "\n",
    "class CompressionModel(transformers.T5ForConditionalGeneration):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.critic_head = nn.Linear(config.d_model, 1)\n",
    "        self.critic_head.weight.data.normal_(mean=0.0, std=(1 / config.d_model))\n",
    "        self.critic_head.bias.data.zero_()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        decoder_input_ids: Optional[torch.LongTensor] = None,\n",
    "        decoder_attention_mask: Optional[torch.BoolTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        decoder_head_mask: Optional[torch.FloatTensor] = None,\n",
    "        cross_attn_head_mask: Optional[torch.Tensor] = None,\n",
    "        encoder_outputs: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n",
    "        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = True,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple[torch.FloatTensor], CompressionOutput]:\n",
    "        output = super().forward(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            decoder_attention_mask=decoder_attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            decoder_head_mask=decoder_head_mask,\n",
    "            cross_attn_head_mask=cross_attn_head_mask,\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            decoder_inputs_embeds=decoder_inputs_embeds,\n",
    "            labels=labels,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        if output.decoder_hidden_states is not None:\n",
    "            last_hidden_state = output.decoder_hidden_states[-1]\n",
    "            value_predictions = self.critic_head(last_hidden_state).squeeze(-1)\n",
    "        else:\n",
    "            value_predictions = None\n",
    "\n",
    "        return CompressionOutput(\n",
    "            value_predictions=value_predictions,\n",
    "            logits=output.logits,\n",
    "            past_key_values=output.past_key_values,\n",
    "            decoder_hidden_states=output.decoder_hidden_states,\n",
    "            decoder_attentions=output.decoder_attentions,\n",
    "            cross_attentions=output.cross_attentions,\n",
    "            encoder_last_hidden_state=output.encoder_last_hidden_state,\n",
    "            encoder_hidden_states=output.encoder_hidden_states,\n",
    "            encoder_attentions=output.encoder_attentions,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "XMVtNmiu-30c"
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "import transformers.modeling_outputs\n",
    "\n",
    "\n",
    "class DecompressionConfig(transformers.T5Config): ...\n",
    "\n",
    "\n",
    "class DecompressionModel(transformers.T5ForConditionalGeneration): ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-OTuhuS295RZ",
    "outputId": "80eb955b-6f1f-4899-df53-f06c5bcda115"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_PATH = Path(\"./data/models/token-dethcod/a2c-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from google-t5/t5-small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CompressionModel were not initialized from the model checkpoint at google-t5/t5-small and are newly initialized: ['critic_head.bias', 'critic_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "LOAD_LATEST = False\n",
    "\n",
    "if LOAD_LATEST:\n",
    "    compressor = CompressionModel.from_pretrained(MODEL_PATH / \"compressor\").to(device)\n",
    "    decompressor = DecompressionModel.from_pretrained(MODEL_PATH / \"decompressor\").to(device)\n",
    "\n",
    "else:\n",
    "    model_path = \"google-t5/t5-small\"\n",
    "    print(f\"Loading model from {model_path}\")\n",
    "    compressor = CompressionModel.from_pretrained(model_path).to(device)\n",
    "    compressor.critic_head.reset_parameters()\n",
    "    decompressor = DecompressionModel.from_pretrained(model_path).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WeKAyrQz5k_k"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "e-neGcFgTHdu"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/khodabandeh/.netrc\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import wandb\n",
    "\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    # Load environment variables from .env file\n",
    "    load_dotenv()\n",
    "\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing dotenv: {e}\")\n",
    "\n",
    "\n",
    "# Check if running in Colab\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    # If running in Colab, use userdata.get to retrieve the token\n",
    "    wandb.login(key=userdata.get('wandb_token'))\n",
    "\n",
    "except ImportError:\n",
    "    # If not in Colab, load the token from the environment variable\n",
    "    wandb_token = os.getenv('WANDB_TOKEN')\n",
    "    if wandb_token:\n",
    "        wandb.login(key=wandb_token, relogin=True)\n",
    "    else:\n",
    "        print(\"W&B token not found in environment variable. Please set WANDB_TOKEN in your environment.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "nbJccLQa_TKV"
   },
   "outputs": [],
   "source": [
    "COMPRESSOR_LR = 1e-3\n",
    "DECOMPRESSOR_LR = 1e-3\n",
    "# CRITIC_BIAS_LR = 0.1\n",
    "\n",
    "# # Create parameter groups\n",
    "# param_groups = [\n",
    "#     {\"params\": [param for name, param in compressor.named_parameters() if name != \"critic_head.bias\"], \"lr\": LR},\n",
    "#     {\"params\": [compressor.critic_head.bias], \"lr\": CRITIC_BIAS_LR},\n",
    "# ]\n",
    "\n",
    "# # Define optimizer with parameter groups\n",
    "# compressor_optimizer = torch.optim.Adam(param_groups)\n",
    "\n",
    "compressor_optimizer = torch.optim.Adam(compressor.parameters(), lr=COMPRESSOR_LR)\n",
    "decompressor_optimizer = torch.optim.Adam(decompressor.parameters(), lr=DECOMPRESSOR_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "zioTdU4gA2J2"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "MAX_TOKEN_COST = math.log(compressor.config.vocab_size)\n",
    "\n",
    "train_dataset = dataset\n",
    "data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "SCHEDULING_STEPS = len(data_loader) * 1.0e-2 # Schedule over 30% of an epoch\n",
    "PRETRAINING_STEPS = len(data_loader) * 2.0e-2 # Schedule over 10% of an epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "SUo_c6cyTx2Y"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maxiom\u001b[0m (\u001b[33mchihuahuas\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76c73572c8c04c85996f111db45c9507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01111310427594516, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/khodabandeh/Projects/dethcod/wandb/run-20241022_235453-c9lkipbq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/chihuahuas/DETHCOD/runs/c9lkipbq' target=\"_blank\">Token Training</a></strong> to <a href='https://wandb.ai/chihuahuas/DETHCOD' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/chihuahuas/DETHCOD' target=\"_blank\">https://wandb.ai/chihuahuas/DETHCOD</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/chihuahuas/DETHCOD/runs/c9lkipbq' target=\"_blank\">https://wandb.ai/chihuahuas/DETHCOD/runs/c9lkipbq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/chihuahuas/DETHCOD/runs/c9lkipbq?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f7bb1945640>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(\n",
    "    name = \"Token Training\",\n",
    "    project=\"DETHCOD\",\n",
    "    config={\n",
    "        \"compressor_model_config\": compressor.config.to_dict(),\n",
    "        \"decompressor_model_config\": decompressor.config.to_dict(),\n",
    "        # TODO: Add other parameters\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenCostScheduler:\n",
    "    def __init__(self, total_steps, max_token_cost, schedule_fn=None):\n",
    "        self.total_steps = total_steps\n",
    "        self.max_token_cost = max_token_cost\n",
    "        self.step_count = 0\n",
    "\n",
    "        linear_schedule = lambda self: min(self.step_count / self.total_steps, 1.0) * self.max_token_cost\n",
    "        # If no schedule function is provided, default to linear schedule\n",
    "        self.schedule_fn = schedule_fn if schedule_fn else linear_schedule\n",
    "\n",
    "    def get_token_cost(self):\n",
    "        # Get the current token cost based on the schedule\n",
    "        token_cost = self.schedule_fn(self)\n",
    "        self.step_count += 1  # Increment the step count\n",
    "        return token_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n"
     ]
    }
   ],
   "source": [
    "graph = wandb.watch((compressor.critic_head, compressor.lm_head), log_freq=100, log=\"all\", log_graph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RL Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388,
     "referenced_widgets": [
      "a0678d2428ff4198a9381b664dea08c4",
      "579d2c20323a4e14aa0b2fcc125e9eb9",
      "1a79263283254ba3a8e909ea3966e3ac",
      "e4dcb7f644ce4739b255809f9df0bf53",
      "584d3bd4ac484269bcf7da1d48c78448",
      "e648ad71485b482992d3b5b699fdf90b",
      "a7c81547e14040a28b9f3e8f469bc933",
      "460431870cd346caa15e02b9ffe0c63f",
      "a75e62964f224e8ab32e9136c00f786e",
      "6f20c7b7089045ffa62fda32898dd673",
      "e79223029d27455288e6daa49aabe4f3"
     ]
    },
    "id": "-71bvb9b4Rth",
    "outputId": "058aeb31-300b-4aef-e930-5421113cbff1"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf9c4234e3064e05a1a52dc867a4c4bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/106887 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import tqdm.auto as tqdm\n",
    "from transformers import GenerationConfig\n",
    "from transformers import modeling_outputs\n",
    "\n",
    "\n",
    "# Define your generation configuration as before\n",
    "generation_config = GenerationConfig(\n",
    "    do_sample=True,\n",
    "    num_beams=1,\n",
    "    max_new_tokens=128,\n",
    "    decoder_start_token_id=compressor.generation_config.decoder_start_token_id,\n",
    "    eos_token_id=compressor.generation_config.eos_token_id,\n",
    "    pad_token_id=compressor.generation_config.pad_token_id,\n",
    "    return_dict_in_generate=True,\n",
    "    output_logits=True,\n",
    ")\n",
    "\n",
    "# Initialize the scheduler\n",
    "token_cost_scheduler = TokenCostScheduler(total_steps=SCHEDULING_STEPS, max_token_cost=MAX_TOKEN_COST)\n",
    "\n",
    "with tqdm.tqdm(data_loader) as pbar:\n",
    "    for step, batch in enumerate(pbar):\n",
    "        # Get the current token cost from the scheduler\n",
    "        token_cost = token_cost_scheduler.get_token_cost()\n",
    "\n",
    "        input_ids = tokenizer(\n",
    "            batch[\"text\"],\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            # TODO: Test if this has any effect\n",
    "            truncation=True,\n",
    "        ).input_ids.to(device)\n",
    "\n",
    "        compressed = compressor.generate(input_ids=input_ids, generation_config=generation_config)\n",
    "        decompressed = decompressor.forward(input_ids=compressed.sequences, labels=input_ids)\n",
    "\n",
    "        # Force last token to be eos for episodes with no eos (terminated by max_len)\n",
    "        full_episodes = (compressed.sequences != generation_config.eos_token_id).all(dim=-1)\n",
    "        sequences_copy = compressed.sequences.clone()\n",
    "        sequences_copy[..., full_episodes, -1] = generation_config.eos_token_id\n",
    "        compressed.sequences = sequences_copy\n",
    "\n",
    "        actions = compressed.sequences[..., 1:]\n",
    "        # compressed.logits: [\n",
    "        #      torch.tensor(shape=(B, V))\n",
    "        # ]\n",
    "        # (L, B, V)\n",
    "        # (B, L, V)\n",
    "        action_distributions = torch.stack(compressed.logits).transpose(0, 1)\n",
    "        # TODO: Give the `actions` as decoder_input_ids instead\n",
    "        values = compressor.forward(input_ids=input_ids, decoder_input_ids=compressed.sequences).value_predictions[..., :-1]\n",
    "        action_mask = actions != generation_config.pad_token_id\n",
    "        is_pad = actions == generation_config.pad_token_id\n",
    "        is_eos = actions == generation_config.eos_token_id\n",
    "        compressed_length = actions.size(-1) - is_pad.logical_or(is_eos).sum(dim=-1)\n",
    "\n",
    "        losses = F.cross_entropy(\n",
    "            decompressed.logits.flatten(0, -2),\n",
    "            target=input_ids.flatten(),\n",
    "            ignore_index=0,\n",
    "            reduction=\"none\",\n",
    "        ).view(input_ids.shape)\n",
    "        decompressor_loss = losses.mean()\n",
    "\n",
    "        sequence_compression_loss = losses.detach().sum(dim=-1)\n",
    "        rewards = torch.where(\n",
    "            actions == generation_config.eos_token_id,\n",
    "            -sequence_compression_loss.unsqueeze(-1),\n",
    "            -token_cost,\n",
    "        ) * action_mask * 0.01\n",
    "        qs = rewards.flip(dims=[-1]).cumsum(dim=-1).flip(dims=[-1])\n",
    "\n",
    "        advantage = (qs - values) * action_mask\n",
    "        masked_advantage = advantage[action_mask]\n",
    "        critic_loss = (masked_advantage * masked_advantage).mean()\n",
    "\n",
    "        compressed_size = (action_mask.sum(dim=-1) - 1) * MAX_TOKEN_COST + sequence_compression_loss\n",
    "        decompressed_size = ((input_ids != 0).sum(dim=-1) - 1) * MAX_TOKEN_COST\n",
    "        compression_ratio = (decompressed_size / compressed_size).mean()\n",
    "\n",
    "        if step < PRETRAINING_STEPS:\n",
    "            # Train the model to generate the original sequence\n",
    "            actor_loss = super(CompressionModel, compressor).forward(input_ids=input_ids, labels=input_ids).loss\n",
    "\n",
    "        else:\n",
    "            # [x] | x \\in R\n",
    "            # b = -ln(\\sigma e^x)\n",
    "            # norm = [x + b][action]\n",
    "            # al = x[action] - ln(sigma(e^x))\n",
    "            #    = ln(e^x[action]) - ln(sigma(e^x))\n",
    "            #    = ln(e^x[action]/sigma(e^x))\n",
    "\n",
    "            # cross entropy = -ln(e^x[action]/sigma(e^x))\n",
    "            action_logits = F.cross_entropy(\n",
    "                action_distributions.flatten(0, -2),\n",
    "                target=actions.flatten(),\n",
    "                ignore_index=0,\n",
    "                reduction=\"none\",\n",
    "            ).view(actions.shape)\n",
    "            actor_loss = (action_logits * advantage.detach()).mean()\n",
    "\n",
    "        compressor_loss = actor_loss + critic_loss\n",
    "\n",
    "        pbar.set_description(f\"{compression_ratio=:.2f}, {critic_loss=:.2f}, {actor_loss=:.2f}, {decompressor_loss=:.2f}\")\n",
    "\n",
    "        compressor_optimizer.zero_grad()\n",
    "        compressor_loss.backward()\n",
    "        compressor_optimizer.step()\n",
    "\n",
    "        decompressor_optimizer.zero_grad()\n",
    "        decompressor_loss.backward()\n",
    "        decompressor_optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            wandb.log(\n",
    "                {\n",
    "                    \"actor_loss\": actor_loss,\n",
    "                    \"critic_loss\": critic_loss,\n",
    "                    \"reward\": rewards.sum(dim=-1).mean(),\n",
    "                    \"decompressor_loss\": decompressor_loss,\n",
    "                    \"accuracy\": (-sequence_compression_loss).exp().mean(),\n",
    "                    \"compressed_size\": compressed_length.float().mean(),\n",
    "                    \"compression_ratio\": compression_ratio,\n",
    "                    \"expected_advantage\": masked_advantage.mean(),\n",
    "                    \"advantage_std\": masked_advantage.std(),\n",
    "                    \"advantage\": masked_advantage,\n",
    "                    \"token_cost\": token_cost,\n",
    "                }\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressor.save_pretrained(MODEL_PATH / \"compressor-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "decompressor.save_pretrained(MODEL_PATH / \"decompressor-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "top_token_count = 100\n",
    "\n",
    "# Assuming `action_distributions` is the tensor of shape [100, 32128]\n",
    "logits = action_distributions[1].detach().cpu()  # Ensure it's on the CPU\n",
    "\n",
    "# Step 1: Average the logits across the first axis (dimension 0)\n",
    "avg_logits = torch.mean(logits, dim=0)\n",
    "\n",
    "# Step 2: Get the top 50 tokens based on average logit values\n",
    "top_values, top_indices = torch.topk(avg_logits, top_token_count)\n",
    "\n",
    "# Step 3: Convert the top indices to tokens using the tokenizer\n",
    "top_tokens = tokenizer.convert_ids_to_tokens(top_indices.numpy())\n",
    "\n",
    "# Step 4: Plot the top 50 logits using imshow with tokens as labels\n",
    "plt.figure(figsize=(10, 2))\n",
    "plt.imshow(logits[..., top_indices].numpy(), cmap='viridis', aspect='auto', interpolation=\"nearest\")\n",
    "plt.colorbar(label='Logit Value')\n",
    "plt.yticks([])  # Hide y-axis as we only have one row\n",
    "plt.xticks(range(top_token_count), top_tokens, rotation='vertical')\n",
    "plt.title('Top 50 Tokens by Average Logit')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "advantage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_tmp = values.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias = nn.Parameter(torch.tensor(0.0, device=device))\n",
    "optim_tmp = torch.optim.Adam(params=[bias])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optim_tmp.param_groups[0]['betas'] = (0.99, 0.5)\n",
    "optim_tmp.param_groups[0]['lr'] = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tqdm.tqdm(range(10000)) as pbar:\n",
    "    for _ in pbar:\n",
    "        advantage = (qs - bias+val_tmp) * action_mask\n",
    "        num_actions = action_mask.sum()\n",
    "        expected_advantage = advantage.sum() / num_actions\n",
    "        critic_loss = (advantage * advantage).sum() / num_actions\n",
    "\n",
    "        optim_tmp.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        optim_tmp.step()\n",
    "\n",
    "        pbar.set_postfix({\n",
    "            \"critic_loss\": critic_loss.item(),\n",
    "            \"bias\": bias.item(),\n",
    "            \"E(adv)\": expected_advantage.item(),\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "advantage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values[0], qs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(advantage[0].cpu().detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eWtEiSe0RZqa",
    "outputId": "e2b87961-8603-48e7-9a72-fecfd1ebec4d"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "sample = random.choice(dataset)\n",
    "print(repr(sample[\"text\"]))\n",
    "\n",
    "input_ids = tokenizer(\n",
    "    batch[\"text\"],\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    ").input_ids.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    compressed = compressor.generate(input_ids=input_ids, generation_config=generation_config)\n",
    "    print(repr(tokenizer.decode(compressed.sequences[0])))\n",
    "    decompressed = decompressor.forward(input_ids=compressed.sequences, labels=input_ids)\n",
    "\n",
    "actions = compressed.sequences[..., 1:]\n",
    "action_distributions = torch.stack(compressed.logits).transpose(0, 1)\n",
    "values = compressor.forward(input_ids=input_ids, decoder_input_ids=compressed.sequences).value_predictions[..., :-1]\n",
    "action_mask = actions != generation_config.pad_token_id\n",
    "is_pad = actions == generation_config.pad_token_id\n",
    "is_eos = actions == generation_config.eos_token_id\n",
    "compressed_length = actions.size(-1) - is_pad.logical_or(is_eos).sum(dim=-1)\n",
    "\n",
    "losses = F.cross_entropy(\n",
    "    decompressed.logits.flatten(0, -2),\n",
    "    target=input_ids.flatten(),\n",
    "    ignore_index=0,\n",
    "    reduction=\"none\",\n",
    ").view(input_ids.shape)\n",
    "decompressor_loss = losses.mean()\n",
    "\n",
    "sequence_compression_loss = losses.detach().sum(dim=-1)\n",
    "rewards = torch.where(\n",
    "    actions == generation_config.eos_token_id,\n",
    "    -sequence_compression_loss.unsqueeze(-1),\n",
    "    -TOKEN_COST,\n",
    ") * action_mask\n",
    "qs = rewards.flip(dims=[-1]).cumsum(dim=-1).flip(dims=[-1])\n",
    "\n",
    "advantage = (qs - values) * action_mask\n",
    "critic_loss = (advantage * advantage).mean()\n",
    "\n",
    "action_logits = F.cross_entropy(\n",
    "    action_distributions.flatten(0, -2),\n",
    "    target=actions.flatten(),\n",
    "    ignore_index=0,\n",
    "    reduction=\"none\",\n",
    ").view(actions.shape)\n",
    "actor_loss = (action_logits * advantage.detach()).mean()\n",
    "\n",
    "print(f\"actor_loss={actor_loss}\")\n",
    "print(f\"critic_loss={critic_loss}\")\n",
    "print(f\"reward={rewards.sum(dim=-1).mean()}\")\n",
    "print(f\"decompressor_loss={decompressor_loss}\")\n",
    "print(f\"accuracy={(-losses.sum(dim=-1)).exp().mean()}\")\n",
    "print(f\"compressed_size={compressed_length.float().mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions[2][4] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_61.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(compressed[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PYiKrtM2M03J",
    "outputId": "ccf98ed3-034e-4363-e687-a08941c63c26"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FLVMLQIqQXCf",
    "outputId": "01a7d128-c502-49f3-9556-fc948ee1f1ef"
   },
   "outputs": [],
   "source": [
    "action_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed[0, 1] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values, indices = compression_output.logits[0, -1].sort(descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.cross_entropy(\n",
    "    compression_output.logits[:, :-1, :].view(-1, num_ids),\n",
    "    target=compressed[:, 1:].flatten(),\n",
    "    ignore_index=0,\n",
    "    reduction='none',\n",
    ") * advantage.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e2T85EARPdwI",
    "outputId": "bc324f8e-e86a-44c4-ab3b-a95c72b1cae3"
   },
   "outputs": [],
   "source": [
    "compression_output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(action_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GtKo3PPGQf0J",
    "outputId": "b0d3ea63-0a51-43ab-8fa2-e784c530ab45"
   },
   "outputs": [],
   "source": [
    "advantage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VXhr1x1tQhII",
    "outputId": "213f761f-3ea8-4f9a-e857-b7e3ac69a167"
   },
   "outputs": [],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fLAz9DwoN5np",
    "outputId": "b3931440-5067-4716-99b2-9421357bd608"
   },
   "outputs": [],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xIKY8_DwNa4l",
    "outputId": "bf6eb955-9a2a-4b57-8db4-6d86a7e94675"
   },
   "outputs": [],
   "source": [
    "len_compressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5zxqYdtCNLn7",
    "outputId": "fb8f9db0-ea93-4a70-be64-dbced88bc02b"
   },
   "outputs": [],
   "source": [
    "advantage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aDveEsAPMsQt",
    "outputId": "b2edf6d5-b6c8-44fa-99b8-13a4321b5afc"
   },
   "outputs": [],
   "source": [
    "actor_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SiJGEKx2MtlU",
    "outputId": "ad04d0e9-3976-418b-a74f-9a480ce1b48b"
   },
   "outputs": [],
   "source": [
    "critic_loss"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1a79263283254ba3a8e909ea3966e3ac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_460431870cd346caa15e02b9ffe0c63f",
      "max": 855090,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a75e62964f224e8ab32e9136c00f786e",
      "value": 1702
     }
    },
    "460431870cd346caa15e02b9ffe0c63f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "579d2c20323a4e14aa0b2fcc125e9eb9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e648ad71485b482992d3b5b699fdf90b",
      "placeholder": "​",
      "style": "IPY_MODEL_a7c81547e14040a28b9f3e8f469bc933",
      "value": "actor_loss=-10.76, critic_loss=745.64, decompressor_loss=1.31:   0%"
     }
    },
    "584d3bd4ac484269bcf7da1d48c78448": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6f20c7b7089045ffa62fda32898dd673": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a0678d2428ff4198a9381b664dea08c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_579d2c20323a4e14aa0b2fcc125e9eb9",
       "IPY_MODEL_1a79263283254ba3a8e909ea3966e3ac",
       "IPY_MODEL_e4dcb7f644ce4739b255809f9df0bf53"
      ],
      "layout": "IPY_MODEL_584d3bd4ac484269bcf7da1d48c78448"
     }
    },
    "a75e62964f224e8ab32e9136c00f786e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a7c81547e14040a28b9f3e8f469bc933": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e4dcb7f644ce4739b255809f9df0bf53": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6f20c7b7089045ffa62fda32898dd673",
      "placeholder": "​",
      "style": "IPY_MODEL_e79223029d27455288e6daa49aabe4f3",
      "value": " 1702/855090 [14:03&lt;68:11:10,  3.48it/s]"
     }
    },
    "e648ad71485b482992d3b5b699fdf90b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e79223029d27455288e6daa49aabe4f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
